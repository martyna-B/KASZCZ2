---
title: "Sprawozdanie 2"
author: "Klaudia Jaworek, Martyna Bielec"
date: "`r Sys.Date()`"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    extra_dependencies: ["polski", "float"]
  word_document: default
---

```{r setup, include = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
knitr::knit_engines$set(python = reticulate::eng_python)
knitr::opts_chunk$set(pythonreticulate = FALSE)
```

```{r include = FALSE}
library(reticulate)
library(aTSA)
library(stats)
library(tseries)
library(nortest)
```

```{python include = FALSE}
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from scipy import stats, optimize
from statsmodels.tsa.stattools import pacf
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.stats.diagnostic import het_arch
from IPython.display import display
import statsmodels.api as sm

sns.set_style("darkgrid")
sns.set_palette(["steelblue", "red"]) 

def autocovariance_estimator(data, h):
    n = len(data)
    h = np.abs(h)
    new_first_data = data[h:]
    new_second_data = data[:n-h]
    mean_data = np.mean(data)
    
    output = (1/n)*sum((new_first_data - mean_data)*(new_second_data - mean_data))
    
    return output

def autocorrelation_estimator(data, h):
  
    data = np.array(data)
    autocovariance = autocovariance_estimator(data, h)
    variance = autocovariance_estimator(data, 0)
    
    return autocovariance/variance
```

# Wstęp

Dane, które poddamy analizie, dotyczą pogody w Londynie w latach 1979-2020. Pomiary były dokonywane codziennie w stacji pogodowej w pobliżu lotniska Heathrow. Zawierają informacje dotyczące m.in. zachmurzenia, nasłonecznienia, ciśnienia oraz temperatury. W sprawozdaniu zajmiemy się analizą maksymalnej temperatury. Sprawdzimy trend deterministyczny i dokonamy dekompozycji, a następnie sprawdzimy jaki szereg czasowy najlepiej modeluje zachowanie danych. Szereg będziemy oznaczać jako $\{X_t\}$.


 
```{python echo = FALSE, fig.cap = "\\label{wykres bez obrobki}Maksymalna temperatura w Londynie.", fig.pos = "H"}

weather_data = pd.read_csv("london_weather.csv")
max_temp_data = weather_data["max_temp"]
date_data = weather_data["date"]
correct_date = [pd.to_datetime(str(date), format='%Y%m%d') for date in date_data]
weather_data["correct_date"] = correct_date
plt.plot(correct_date, max_temp_data)
plt.xlabel("Czas")
plt.ylabel("Maksymalna temperatura")
```

Z wykresu [\ref{wykres bez obrobki}] odczytujemy, że obserwowane wartości wykazują się sezonowością, zgodnie z oczekiwaniami. 


# Przygotowanie danych do analizy

## Wartości brakujące i obserwacje odstające

W danych znajduje się sześć dni, dla których nie zmierzono maksymalnej temperatury. Są to: 05.02, 10.03, 06.05, 16.07, 10.08 oraz 08.10 z 2020 roku. Z tego powodu weźmiemy pod uwagę jedynie dane sprzed 2020 roku. 

```{python include = FALSE}
date_data = date_data[:-365]
max_temp_data = max_temp_data[:-365]
correct_date = correct_date[:-365]
```

Najwyższa zaobserwowana temperatura wyniosła $37.9 ^\circ$, a najniższa $-6.2^\circ$. Wartości są wiarygodne, skąd wnioskujemy, że dane nie zawierają błędów w pomiarach.

## Dekompozycja szeregu czasowego

### ACF oraz PACF dla surowych danych

ACF jest funkcją korelacji między dwoma obserwacjami z szeregu $\{X_t\}$, oddalonymi o $h$, gdzie $h \in \mathbf{Z}$, czyli $\mathrm{corr}(X_t, X_{t+h})$. Estymuje się ją w natępujący sposób:

$$\hat{\rho}(h) = \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)},$$
gdzie $\hat{\gamma}(h)$ jest empiryczną funkcją autokowariancji, wyrażoną wzorem

$$\hat{\gamma}(h) = \frac{1}{n} \sum_{t=1}^{n - |h|} (x_{t+|h|} - \overline{x})(x_t - \overline{x}),$$

gdzie $x_1, x_2, \dots, x_n$ są realizacjami szeregu czasowego $X_t$, a $\overline{x} = \frac{1}{n}\sum_{i=1}^n x_i$.

Obliczamy ACF dla badanego szeregu czasowego, dla $h=0, 1, 2, \dots 20$.

```{python echo = FALSE, fig.cap = "\\label{fig:ACF bez obrobki 20}ACF dla surowych danych dla $h < 21$.", fig.pos = "H"}

H_20 = np.arange(21)
autocorr_20 = [autocorrelation_estimator(max_temp_data, h) for h in H_20]

plt.plot(H_20, autocorr_20)
plt.xlabel("$h$")
plt.ylabel("ACF")
```



```{python echo = FALSE, fig.cap = "\\label{fig:ACF bez obrobki 1000}ACF dla surowych danych dla $h < 1001$.", fig.pos = "H"}

#to pewnie się usunie

H_1000 = np.arange(1001)
autocorr_1000 = [autocorrelation_estimator(max_temp_data, h) for h in H_1000]

plt.plot(H_1000, autocorr_1000)
plt.xlabel("$h$")
plt.ylabel("ACF")
```

PACF jest funkcją częściowej autokorelacji. Służy do określenia bezpośredniej zależności między $X_t$ a $X_{t+h}$. Definiujemy ją w następujący sposób

$$\alpha(0)=1, \alpha(h) = \phi_{hh}, h \ge 1,$$
gdzie $\phi_{hh}$ jest ostatnią składową $\phi_h = \Gamma_h^{-1}\gamma_h,$ gdzie $\Gamma_h = [\gamma(i-j)]^h_{i,j=i}, \gamma_h = [\gamma(1), \gamma(2), \dots, \gamma(h)]'.$

PACF dla danych obliczymy z użyciem funkcji wbudowanej w języku programowania Python.

```{python echo = FALSE, fig.cap = "\\label{fig:PACF bez obrobki 20}PACF dla surowych danych dla $h < 21$.", fig.pos = "H"}

PACF_20 = pacf(max_temp_data, nlags=20)
plt.plot(np.arange(21), PACF_20)
plt.xlabel("$h$")
plt.ylabel("PACF")
```

```{python echo = FALSE, fig.cap = "\\label{fig:PACF bez obrobki 1000}PACF dla surowych danych dla $h < 1001$.", fig.pos = "H"}

PACF_1000 = pacf(max_temp_data, nlags=1000)
plt.plot(np.arange(1001), PACF_1000)
plt.xlabel("$h$")
plt.ylabel("PACF")
```

### Identyfikacja trendów deterministycznych

W danych widać trend liniowy i sezonowość dlatego spodziewamy się, że 

$$X_t = m(t) + s(t) + Y_t,$$
gdzie $m(t)$ to wielomian rzędu 1, $s(t)$ to funkcja okresowa, a ${Y_t}$ to szereg ARMA($p$, $q$).

```{python include = FALSE}
line_fit = np.polyfit(np.arange(len(correct_date)), max_temp_data, 1)
```

```{r nclude = FALSE}
line_fit <- py$line_fit
```

Do estymacji współczynników funkcji $m(t)$ i $s(t)$ wykorzystamy funkcje korzystajace z metody najmniejszych kwadratów zaimplementowane w języku Python. W ten sposób otrzymujemy, że 

$$m(t) \approx `r round(line_fit[1], 6)` t + `r round(line_fit[2], 2)`,$$
gdzie $t$ oznacza liczbę dni, które upłynęły od 1.01.1979r. Dodatni współczynnik kierunkowy prostej $m(t)$ wskazuje, że wraz z upływem czasu rosły maksymalne temperatury powietrza w Londynie. Po analizie wykresu [\ref{fig:dopasowana prosta}] możemy stwierdzić, że dopasowana prosta adekwatnie opisuje ten wzrost temperatur, więc zakładamy, że współczynniki zostały dobrane poprawnie.

```{python echo = FALSE, fig.cap = "\\label{fig:dopasowana prosta}Dopasowanie funkcji liniowej do danych.", fig.pos = "H"}
plt.plot(correct_date, max_temp_data, label = "Dane")
plt.plot(correct_date, np.polyval(line_fit, np.arange(len(correct_date))), label = "Dopasowana prosta")
plt.xlabel("Czas")
plt.ylabel("Maksymalna temperatura")
plt.legend()
```

Wykres [\ref{fig:bez trendu}] przedstawia dane po usunięciu liniowego trendu.

```{python echo = FALSE, fig.cap = "\\label{fig:bez trendu}Maksymalna temperatura w Londynie po usunięciu trendu liniowego.", fig.pos = "H"}
temp_without_trend = max_temp_data - np.polyval(line_fit, np.arange(len(correct_date)))
plt.plot(correct_date, temp_without_trend)
plt.xlabel("Czas")
plt.ylabel("Maksymalna temperatura")
```

```{python include = FALSE}
def my_sin(x, T, amplitude, phase):
    return np.sin(x * 2 * np.pi /T + phase) * amplitude

sin_fit = optimize.curve_fit(my_sin, np.arange(len(correct_date)), temp_without_trend, p0 = [365, 20, 0])[0]
```

```{r include = FALSE}
sin_fit <- py$sin_fit
```

Po estymacji współczynników funkcji $s(t)$ otrzymujemy

$$s(t) \approx `r round(sin_fit[2], 2)`\sin \left( \frac{2\pi t}{`r round(sin_fit[1], 2)`} + `r round(sin_fit[3], 2)`\right),$$
gdzie $t$ podobnie jak poprzednio oznacza liczbę dni, które upłynęły od 1.01.1979r. Po analizie wykresu [\ref{fig:dopasowany sinus}] możemy stwierdzić, że funkcja została poprawnie dobrana. Dane po odjęciu wartości funkcji $m(t)$ i $s(t)$ zostały przedstawione na wykresie [\ref{fig:bez sezonowości}] i wyglądają na losowe, co sugeruje, że całkowicie pozbyliśmy się z nich trendu i sezonowości.

```{python echo = FALSE, fig.cap = "\\label{fig:dopasowany sinus}Dopasowanie funkcji okresowej do danych.", fig.pos = "H"}
fitted_sin = my_sin(np.arange(len(correct_date)), *sin_fit)
plt.plot(correct_date, temp_without_trend, label = "Dane")
plt.plot(correct_date, fitted_sin, label = "Dopasowana funkcja okresowa")
plt.xlabel("Czas")
plt.ylabel("Maksymalna temperatura")
plt.legend()
```

```{python echo = FALSE, fig.cap = "\\label{fig:bez sezonowości}Maksymalna temperatura w Londynie po usunięciu trendu liniowego i sezonowości.", fig.pos = "H"}
temp_without_seasonality = temp_without_trend - fitted_sin
plt.plot(correct_date, temp_without_seasonality)
plt.xlabel("Czas")
plt.ylabel("Maksymalna temperatura")
```

### ACF oraz PACF dla danych po usunięciu trendów deterministycznych

```{python echo = FALSE, fig.cap = "\\label{fig:ACF po obrobce 20}ACF dla danych po usunięciu trendów deterministycznych dla $h < 21$.", fig.pos = "H"}

autocorr_20_after = [autocorrelation_estimator(temp_without_seasonality, h) for h in H_20]

plt.plot(H_20, autocorr_20_after)
plt.xlabel("$h$")
plt.ylabel("ACF")
```

```{python echo = FALSE, fig.cap = "\\label{fig:ACF po obrobce 1000}ACF dla danych po usunięciu trendów deterministycznych dla $h < 1001$.", fig.pos = "H"}

autocorr_1000_after = [autocorrelation_estimator(temp_without_seasonality, h) for h in H_1000]

plt.plot(H_1000, autocorr_1000_after)
plt.xlabel("$h$")
plt.ylabel("ACF")
```

```{python echo = FALSE, fig.cap = "\\label{fig:PACF po obrobce 20}PACF dla danych po usunięciu trendów deterministycznych dla $h < 21$.", fig.pos = "H"}

PACF_20_after = pacf(temp_without_seasonality, nlags=20)
plt.plot(np.arange(21), PACF_20_after)
plt.xlabel("$h$")
plt.ylabel("PACF")
```

```{python echo = FALSE, fig.cap = "\\label{fig:PACF po obrobce 1000}PACF dla danych po usunięciu trendów deterministycznych dla $h < 1001$.", fig.pos = "H"}

PACF_1000_after = pacf(temp_without_seasonality, nlags=1000)
plt.plot(np.arange(1001), PACF_1000_after)
plt.xlabel("$h$")
plt.ylabel("PACF")
```

## Modelowanie danych przy pomocy ARMA

### Dobranie rzędu modelu

Chcemy zamodelować dane szeregiem czasowym ARMA($p$, $q$). W tym celu musimy wyznaczyć optymalne wartości $p$ i $q$. Posłużymy się kryterium informacyjny Akaikego.

Wyliczymy wartość AIC dla parametrów $p \in \{0, 1, \dots, 9\}$ oraz $q \in \{0, 1, \dots, 9\}$.

```{python echo = FALSE}
pq_df = pd.read_csv("pq.csv")

#do zmiany na ładniejszą tabelę  
display(pq_df)
```

AIC przyjmuje najmniejszą wartość dla $p=6, q=0$. Przyjmujemy więc, że badany szereg czasowy $\{X_t\}$ jest ARMA($6$, $0$), czyli AR($6$).

### Estymacja parametrów modelu.

Badany szereg czasowy $\{X_t\}$ jest postaci:

$$X_t - \phi_1 X_{t-1} - \phi_2 X_{t-2} - \dots - \phi_6 X_{t-6} = Z_t,$$
gdzie $Z_t \sim WN(0, \sigma).$ Wyestymujemy parametry $\phi_i$ oraz $\sigma$, korzystając z metody Yule-Walkera.

```{python echo = FALSE}
phi, sigma = sm.regression.yule_walker(max_temp_data, order=6)
```

```{=latex}

\begin{table}[H]
\label{tabela phi}
\begin{center}
\begin{tabular}{|l|l|ll}
\hline
\textbf{$\phi_1$} & 0.7031  & \multicolumn{1}{l|}{\textbf{$\phi_5$}} & \multicolumn{1}{l|}{0.0181} \\ \hline
\textbf{$\phi_2$} & 0.0063  & \multicolumn{1}{l|}{\textbf{$\phi_6$}} & \multicolumn{1}{l|}{0.0155} \\ \hline
\textbf{$\phi_3$} & -0.0023 & \multicolumn{1}{l|}{\textbf{$\sigma$}} & \multicolumn{1}{l|}{2.3298} \\ \hline
\textbf{$\phi_4$} & 0.0156  &                                        &                             \\ \cline{1-2}
\end{tabular}
\end{center}
\caption{Wartości parametrów $\phi_i$ i $\sigma$.}
\end{table}

```

## Ocena dopasowania modelu

### Przedziały ufności dla PACF oraz ACF

Wygenerujemy 1000 trajektorii szeregu czasowego AR($6$) o współczynnikach $\phi_i$ zadanych w tabeli [\ref{tabela phi}]. Następnie dla $h \in \{0, 1, \dots, 20\}$ wyestymujemy dla każdej z nich wartości PACF oraz ACF, aby stworzyć przedziały ufności na poziomie ufności $0.05$.

```{python echo=FALSE, fig.cap = "\\label{fig:ACF przedzialy ufnosci}Przedziały ufności ACF dla $h <21$.", fig.pos = "H"}

lags_acf = pd.read_csv("acf_przedzialy_ufnosci.csv")

lags_acf_arr = np.array(lags_acf)

uppers_acf = []
lowers_acf = []

for lag in lags_acf_arr:
    lags_list = list(lag)
    lags_list.sort()
    uppers_acf.append(lags_list[994])
    lowers_acf.append(lags_list[4])

plt.plot(H_20, lowers_acf, color = "red", alpha=0.5)
plt.plot(H_20, uppers_acf, color = "red", label="Przedziały ufności", alpha=0.5)
plt.scatter(H_20, autocorr_20_after, label="ACF szeregu $\{X_t\}$")
plt.legend()
plt.xlabel("$h$")
plt.ylabel("ACF")

```

```{python echo=FALSE, fig.cap = "\\label{fig:PACF przedzialy ufnosci}Przedziały ufności PACF dla $h <21$.", fig.pos = "H"}

lags_pacf = pd.read_csv("pacf_przedzialy_ufnosci.csv")

lags_pacf_arr = np.array(lags_pacf)

uppers_pacf = []
lowers_pacf = []

for lag in lags_pacf_arr:
    lags_list = list(lag)
    lags_list.sort()
    uppers_pacf.append(lags_list[994])
    lowers_pacf.append(lags_list[4])

plt.plot(H_20, lowers_pacf, color = "red", alpha=0.5)
plt.plot(H_20, uppers_pacf, color = "red", label="Przedziały ufności", alpha=0.5)
plt.scatter(H_20, PACF_20_after, label="PACF szeregu $\{X_t\}$")
plt.legend()
plt.xlabel("$h$")
plt.ylabel("PACF")

```

### Porównanie linii kwantylowych z trajektorią

Wykorzystamy 1000 tajektorii wygenerowanych wcześniej szeregów, aby stworzyć linie kwantylowe dla kwantyli

# Weryfikacja założeń dotyczących szumu

```{python include = FALSE}
p = 6
q = 0
model = ARIMA(temp_without_seasonality, order = (p, 0, q)).fit(method = "yule_walker")
residuals = model.resid
```

## Założenie dotyczące średniej

```{r include = FALSE}
pvalue_ttest <- t.test(py$residuals)$p.value
```

Wykres [\ref{fig:residua średnia}] przedstawia wartości residuów w zależności od numeru obserwacji. Wartości wyglądają na rozłożone symetrycznie względem zera, a ich średnia wynosi `r mean(py$residuals)`. Do sprawdzenia, czy jest ona istotnie różna od zera wykorzystamy test t-Studenta. Przyjmujemy poziom istotności $\alpha = 0.05$. P-wartość w$~$przeprowadzonym teście wynosi `r pvalue_ttest`, więc nie ma podstaw do odrzucenia hipotezy zerowej. Będziemy zakładać, że residua mają średnią równą $0$.

```{python echo = FALSE, fig.cap = "\\label{fig:residua średnia}Wykres wartości resztowych.", fig.pos = "H"}
plt.scatter(np.arange(len(correct_date)), residuals, alpha = 0.3)
plt.axhline(0, color = "red")
plt.xlabel("Numer obserwacji")
plt.ylabel("Wartości resztowe")
```


## Założenie dotyczące wariancji

```{python include = FALSE}
pvalue_wariancja = het_arch(residuals)[1]
```

```{r include = FALSE}
pvalue_wariancja <- py$pvalue_wariancja
```

Na wykresie [\ref{fig:residua średnia}] nie są zauważalne znaczące zmiany w wielkości rozproszenia. Możemy wykonać test ARCH weryfikujący hipotezę o stałości wariancji. Przyjmujemy poziom istotności $\alpha = 0.05$. P-wartość w przeprowadzonym teście wynosi `r pvalue_wariancja`, więc odrzucamy hipotezę o stałości wariancji wartości resztowych.

## Założenie dotyczące niezależności

```{r include = FALSE}
pvalue_ljungbox <- Box.test(py$residuals, lag = 1, type = "Ljung-Box", fitdf = 0)$p.value
```

Na podstawie wykresu [\ref{fig:ACF residua}] możemy przypuszczać, że residua są niezależne, bo wartości funkcji empirycznej autokorelacji są bliskie zeru dla $h \neq 1$. Dodatkowo możemy przeprowadzić test Ljunga-Boxa sprawdzający prawdziwość hipotezy zerowej o niezależności danych. Przyjmujemy poziom istotności $\alpha = 0.05$. P-wartość w przeprowadzonym teście wynosi `r pvalue_ljungbox`, więc nie ma podstaw do odrzucenia hipotezy zerowej. Możemy założyć, że residua są niezależne.

```{python echo = FALSE, fig.cap = "\\label{fig:ACF residua}Funkcja empirycznej autokorelacji dla wartości resztkowych.", fig.pos = "H"}

autocorr_residuals = [autocorrelation_estimator(residuals, h) for h in H_20]

plt.scatter(H_20, autocorr_residuals)
plt.xlabel("$h$")
plt.ylabel("ACF")
```


## Założenie dotyczące normalności rozkładu

```{r include = FALSE}
pvalue_normtest <- jarque.bera.test(py$residuals)$p.value
pvalue_normtest2 <- ks.test(py$residuals, "pnorm", mean(py$residuals), sd(py$residuals))$p.value
pvalue_normtest3 <- lillie.test(py$residuals)$p.value
```

Na wykresie [\ref{fig:residua hist}] widać niewielkie różnice między gęstością empiryczną a teoretyczną gęstością rozkładu normalnego, natomiast punkty na wykresie [\ref{fig:qqplot}] układają się w prawie prostą linię. Do zweryfikowania hipotezy o normalności rozkładu wykorzystamy test Kołmogorowa-Smirnowa. Przyjmujemy poziom istotności $\alpha = 0.05$. P-wartość w przeprowadzonym teście wynosi `r pvalue_normtest2`, więc odrzucamy hipotezę o rozkładzie normalnym wartości resztowych.

```{r, echo=FALSE, fig.cap = "\\label{fig:residua hist}Porównanie empirycznej i teoretycznej gęstości dla wartości resztowych.",  fig.align='center', fig.pos="H"}
knitr::include_graphics("resid_density.pdf")
```

```{python include = FALSE}
f, ax = plt.subplots(1)
sns.histplot(x = residuals, ax = ax, kde = True, stat = "density", bins = 20, label = "Gęstość empiryczna")
sns.lineplot(x = np.arange(-10, 10, 0.001), y = stats.norm.pdf(np.arange(-10, 10, 0.001), np.mean(residuals), np.std(residuals)), color = "red", label = "Gęstość rozkładu normalnego")
ax.set(ylabel = "Gęstość")
plt.savefig('resid_density.pdf')
```

```{r, echo=FALSE, fig.cap = "\\label{fig:qqplot}Wykres kwantylowy -- porównanie kwantyli empirycznych wartości resztowych z kwantylami rozkładu normalnego.",  fig.align='center', fig.pos="H"}
knitr::include_graphics("qqplot.pdf")
```

```{python include = FALSE}
f, ax = plt.subplots(1)
sm.qqplot(residuals, ax = ax)
ax.set(xlabel = "Kwantyle teoretyczne", ylabel = "Kwantyle empiryczne")
plt.savefig("qqplot.pdf")
```


# Wnioski -- podsumowanie
